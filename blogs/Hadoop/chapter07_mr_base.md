# How MapReduce Works

### Anatomy of a MapReduce Job Run
* **Job Submission**: (1) The `submit()` method on `Job` creates an internal `JobSubmitter` instance and calls `submitJobInternal()` on it. (2) Asks the resource manager for a new application ID, used for the MapReduce job ID. (3) Checks the output specification. Computes the input splits for the job. Copies the resources needed to run the job, including the job JAR file, the configuration file, and the computed input splits, to the shared filesystem in a directory named after the job ID. (4) Submits the job by calling `submitApplication()` on the resource manager. 
* **Job Initialization**: (5) Resource manager transfers the `submitApplication()` request to YARN scheduler. The scheduler allocates a container, and the resource manager then launches the app master's process there, under the node manager's management. (6) The app master is a Java app whose main class is MRAppMAster. It initializes the job by creating a number of bookkeeping objects. (7) It retrieves the input splits computed in the client from the shared filesystem. 
* **Task Assignment**: (8) App master requests containers for all the map and reduce tasks in the job from the resource manager. 
* **Task Execution**: (9) Once a task has been assigned resources for a container on a particular node by the resource manager’s scheduler, the app master starts the container by contacting the node manager. (10) The task is executed by a Java application whose main class is YarnChild. it localizes the resources that the task needs (JAR, conf, ...). (11) Run. 
> **Streaming**: The Streaming task communicates with the process using standard input and output streams. 
* **Progress and Status Updates**: When a task is running, it keeps track of its progress. The child process communicates with its parent app master through the `umbilical` interface. The client receives the latest status by polling the app master every second. 
* **Job Completion**: When the app master receives a notification that the last task for a job is complete, it changes the status for the job to “successful.” Then, when the Job polls for status, it learns that the job has completed successfully, so it prints a message to tell the user and then returns from the `waitForCompletion()` method. 

### Failures
* **Task Failure**: 
> If this happens, the task JVM reports the error back to its parent app master before it exits.  
> For Streaming tasks, if the Streaming process exits with a nonzero exit code, it is marked as failed.   
> Another failure mode is the sudden exit of the task JVM. In this case, the node manager notices that the process has exited and informs the app master so it can mark the attempt as failed. When the app master is notified of a task attempt that has failed, it will reschedule execution of the task.   
> Furthermore, if a task fails four times (`mapreduce.map.maxattempts`), it will not be retried again.    
> A task attempt may also be killed. Killed task attempts do not count against the number of attempts to run the task.    
> Users may also kill or fail task attempts using the web UI or the command line.
* **App Master Failure**: applications in YARN are retried in the event of failure. The maximum number of attempts to run a MapReduce app master is controlled by the `mapreduce.am.max-attempts` property. The limit of any YARN app master running on the cluster is set by `yarn.resourcemanager.am.max-attempts`. An app master sends periodic heartbeats to the resource manager, and in the event of app master failure, the resource manager will detect the failure and start a new instance of the master running in a new container (managed by a node manager). 
* **Node Manager Failure**: If a node manager fails by crashing or running very slowly, it will stop sending heartbeats to the resource manager. The resource manager will notice a node manager that has stopped sending heartbeats if it hasn’t received one for 10 minutes and remove it from its pool of nodes to schedule containers on. Node managers may be blacklisted if the number of failures(`mapreduce.job.maxtaskfailures.per.tracker`) for the application is high. Blacklisting is done by the application master. 
* **Resource Manager Failure**: neither jobs nor task containers can be launched. 

### Shuffle and Sort
* **The Map Side**: Map task writes output to a circular memory buffer (`mapreduce.task.io.sort.mb property`). When the content of the buffer reach the threshold size (`mapre duce.map.sort.spill.percent`), a background thread will start spill the contents to disk (`mapreduce.cluster.local.dir`) in round-robin fashion. Before it writes to disk, the thread first divides the data into partitions, and performs an in-memory sort by key for each partition, and runs the combiner on output if a combiner is defined. After map task has written its last output record, there could be several spill files. The spill files are merged into a single partitioned and sorted output file. If there are at least 3 spill files (`mapreduce.map.combine.minspills`), the combiner is run repeatly over the output until only 1 or 2 spills left. The output files are transferred over HTTP. 
* **The Reduce Side**: The reduce task starts copying map task outputs as soon as each completes. Data is stored in memory or disk (`mapreduce.reduce.shuffle.input.buffer.percent`). When the in-memory buffer reaches a threshold (`mapreduce.reduce.shuffle.merge.percent`) or reaches a threshold number of map outputs (`mapreduce.reduce.merge.inmem.threshold`), it is merged and spilled to disk. As the copies accumulate on disk, a background thread merges them into larger, sorted files. When all the map outputs have been copied, the reduce merges the map outputs, maintaining their sort ordering. The final round, which is supposed to merge the last intermediate file, feeds all intermediate files and the rest of map outputs to the reduce function. The output of reduce phase is written directly to the output filesystem. 
* **Configuration Tuning**: general principles: give the shuffle as much memory as possible (make sure that your map and reduce functions get enough memory to operate), minimize the number of spills, reserve all the memory for the reduce function, increase buffer size (4 KB by default) across the cluster. 

### Task Execution
* **The Task Execution Environment**: the properties can be accessed from the job's configuration. 
> **Streaming environment variables**: it replaces nonalphanumeric characters with underscores to make sure they are valid names. e.g. `mapreduce.job.id` perperty is `os.environ["mapreduce_job_id"]` in Python Streaming script. 
* **Speculative Execution**: The MapReduce model is to break jobs into tasks and run the tasks in parallel to make the overall job execution time smaller than it would be if the tasks ran sequentially. Hadoop tries to detect when a task is running slower than expected and launches another equivalent task as a backup. 
* **Output Committers**: Hadoop MapReduce uses a commit protocol to ensure that jobs and tasks either succeed or fail cleanly. `OutputCommitter` APIs: `setupJob()` method is called before the job is run. If the job succeeds, the `commitJob()` method is called. Otherwise, `abortJob()` is called with a state object indicating whether the job failed or was killed. At the task level, `setupTask()` method is called before the task is run. `needsTaskCommit()` determines whether or not to commit the task. If a task succeeds, `commitTask()` is called. Otherwise, the framework calls `abortTask()`. The framework ensures only one task will be committed in event of multiple task attempts. 
> **Task side-effect files**: If applications write side files in their tasks’ working directories, the side files for tasks that successfully complete will be promoted to the output directory automatically, whereas failed tasks will have their side files deleted.  
